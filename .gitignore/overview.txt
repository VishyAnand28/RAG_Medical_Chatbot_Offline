Repo Layout / Project Directory Tree

aok-rag-bot/
â”œâ”€ app/
â”‚  â”œâ”€ api/                     # FastAPI endpoints - Not done
â”‚  â”œâ”€ rag/
â”‚  â”‚  â”œâ”€ graph.py              # LangGraph: nodes/edges
â”‚  â”‚  â”œâ”€ retriever.py          # semantic search + rerank
â”‚  â”‚  â”œâ”€ prompts.py            # system & answer templates (German)
â”‚  â”‚  â””â”€ guardrails.py         # safety & escalation rules
â”‚  â””â”€ ui/
â”‚     â””â”€ streamlit_app.py
â”œâ”€ data/
â”‚  â”œâ”€ raw 
â”‚  â”œâ”€ processed 
â”‚  â”œâ”€ seed_sources.yaml        # curated crawl targets
â”‚  â”œâ”€ faq_de.yaml              # 10-topic paraphrased FAQ
â”‚  â””â”€ docs/                    # (optional) user-provided PDFs
â”œâ”€ ingest/
â”‚  â”œâ”€ ingest.py                # loads YAML, fetches, cleans, chunks, upserts/ # scrape â†’ clean â†’ chunk â†’ embed â†’ upsert
â”‚  â””â”€ cleaners.py              # HTML/PDF cleaners (nav stripping etc.)
â”œâ”€ models/
â”‚  â””â”€ README.md                # how to pull HF models locally (offline cache)
â”œâ”€ docker/
â”‚  â”œâ”€ Dockerfile.app           # Not done
â”‚  â”œâ”€ docker-compose.yml       # Not done
â”‚  â””â”€ requirements.txt         # Not done
â”œâ”€ scripts/
â”‚  â”œâ”€ download_models.sh       # prefetch embeddings/reranker - Not done
â”‚  â””â”€ build_index.sh           # wrapper for ingest.py - Not done
â”œâ”€ tests/
â”‚  â”œâ”€ build_evalset_from_faq.py # make eval_set.jsonl from FAQ
â”‚  â”œâ”€ collect_answers.py        # run your pipeline, save answers+contexts
â”‚  â”œâ”€ ragas_eval.py             # compute RAGAS metrics (faithfulness, etc.)
â”‚  â””â”€ ragas_scores.csv          # (generated): per-sample & summary scores
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â””â”€ README.md

Ingest HTML/PDF â†’ chunk â†’ embed â†’ vector DB
RAG with LangChain + LangGraph (no OpenAI)
Embeddings & reranker via HuggingFace
LLM via Ollama (local)
Display via Streamlit or Gradio
Serve API via FastAPI + Uvicorn
Eval via RAGAS
Containerize via Docker (so freeze Python deps here)

Ingest: python -m ingest.ingest --manifest data/seed_sources.yaml --faq data/faq_de.yaml
Smoke Test: python scripts/smoke_retrieval.py; python -m scripts.smoke_hybrid
python -m streamlit run ui/streamlit_app.py

python -m scripts.smoke_graph
# Try:
# - "Notfall, starke Brustschmerzen â€“ was tun?"
# - "Wie Ã¤ndere ich meine Bankverbindung?"
# - "Schreib mir ein Python-Programm ..."

RAGAS:
python -m tests.build_evalset_from_faq
python -m tests.collect_answers
python -m tests.ragas_eval

Ingest:
cleaners.py
	small helpers to normalize text (strip menus, boilerplate, artifacts from HTML/PDF).
	optional chunker (split text into ~500â€“1000 tokens so embeddings work).

ingest.py
	entrypoint script for the pipeline.
	does these steps when you run it:
	load sources (seed_sources.yaml, faq_de.yaml, or PDFs from data/docs).
	fetch/parse HTML or PDF â†’ raw text.
	clean & chunk (via cleaners.py).
	embed chunks with HuggingFace model (BAAI/bge-m3).
	upsert into Chroma (your vector database at data/processed/chroma).
	(optionally) dump chunks into a JSONL file â†’ later used for BM25 hybrid retrieval.

rag

retriever.py â€” Hybrid retrieval (+ optional rerank)
	What it does: Finds the best context chunks for a user question.
How:
	Dense: Chroma vector store (persisted at data/processed/chroma) using BAAI/bge-m3.
	Sparse: BM25 over a JSONL corpus (data/processed/chunks.jsonl) dumped by the ingest step.
	Fusion: EnsembleRetriever combines BM25 + dense (weights ~0.45/0.55).
	(Optional) Reranker: BAAI/bge-reranker-v2-m3 cross-encoder reorders the fused results.
	Tuning knobs: k_pool, weights, top_k, and reranker on/off.

prompts.py â€” System prompt & formatting
	What it does: Builds the German instruction prompt passed to the LLM.
	Rules baked in:
	Answer only from provided context; if insufficient, say you have no evidence.
	No made-up URLs; be concise (3â€“6 sentences).
	The model must not print its own sources list (we append curated citations later).

guardrails.py â€” Simple safety / routing
	What it does: Lightweight guardrails before generation.
	Now: is_emergency() detects acute/medical-emergency hints; returns a stock escalation message (112 / Clarimedis) instead of generating.
	graph.py â€” LangGraph pipeline (retrieve â†’ generate)
	What it does: Orchestrates the flow with a small state machine.
	Nodes:
	route â†’ decides â€œemergency?â€ (then end) or proceed to retrieval.

retrieve â†’ calls HybridRetriever (BM25 + dense + optional rerank) and stores top-k docs.
generate â†’ builds the prompt with contexts, calls Ollama (llama3.2:3b), strips any model-made â€œQuellenâ€, and appends our citations (titles + URLs from retrieved docs).
Resource-safe settings: temperature=0.2, smaller context (e.g., num_ctx omitted or ~4096) and num_predict=256 to fit your RAM.

What we validated
	Ingestion & index built successfully (Chroma + BM25 JSONL).
	Hybrid retrieval returns relevant AOK/GKV pages + FAQ chunks.
	End-to-end RAG answers render correctly with a single curated Quellen block.
	Ollama model confirmed (llama3.2:3b, chat/instruct template present).

Perfect time to zoom out ğŸš€ â€” youâ€™ve already built ingestion + hybrid retrieval, now letâ€™s map the **tooling landscape for RAG** so you know what each piece is, what options exist, and whatâ€™s â€œbestâ€ in 2025.

---

# ğŸ§© Core Components in a RAG System

## 1. **Embedder** (turns text â†’ vectors)

* **What it does**: maps chunks of text into numerical vectors so â€œsemantic similarityâ€ can be computed.
* **Popular options**:

  * ğŸ”¹ **HuggingFace sentence-transformers** (e.g., `BAAI/bge-m3`, `all-MiniLM-L6-v2`) â†’ free, local, flexible.
  * ğŸ”¹ **OpenAI embeddings** (`text-embedding-3-small/large`) â†’ very high quality, but requires API + cost.
  * ğŸ”¹ **Cohere Embed**, **Jina AI**, **Voyage AI** â†’ commercial, often stronger multilingual.
* **Best choice**:

  * âœ… For **offline, healthcare-sensitive, and German content** â†’ `BAAI/bge-m3` (multi-lingual, strong benchmarks).
  * âŒ OpenAI API if you need local-only.

---

## 2. **Retriever** (how we fetch candidate docs)

* **What it does**: finds the most relevant chunks for a query.
* **Types**:

  * ğŸ”¹ **Dense retrievers** â†’ use only embeddings (e.g., Chroma, FAISS, Weaviate).
  * ğŸ”¹ **Sparse retrievers** â†’ keyword-based (BM25, ElasticSearch, Whoosh).
  * ğŸ”¹ **Hybrid retrievers** â†’ fuse both (LangChain `EnsembleRetriever`).
  * ğŸ”¹ **Advanced** â†’ ColBERT, SPLADE (neural retrievers).
* **Best choice**:

  * âœ… Hybrid (BM25 + dense) â†’ balances precision (keywords) + recall (semantics).
  * âœ… Add reranking on top for quality.

---

## 3. **Reranker** (re-scores retrieval results)

* **What it does**: takes top-k candidate chunks, and a cross-encoder model re-evaluates them in context of the query.
* **Popular models**:

  * ğŸ”¹ `BAAI/bge-reranker-v2-m3` (HuggingFace) â†’ SOTA, multilingual, local.
  * ğŸ”¹ `cross-encoder/ms-marco-MiniLM-L-6-v2` â†’ smaller, lighter, weaker.
  * ğŸ”¹ OpenAI `text-embedding-ada-002` style reranking (via API).
* **Best choice**:

  * âœ… `bge-reranker-v2-m3` if you have GPU / enough RAM (\~2.3 GB).
  * âŒ skip reranker if resource-constrained, but retrieval quality drops.

---

## 4. **Vector Database (VDB)** (stores embeddings)

* **What it does**: stores vectors and performs similarity search (ANN: approximate nearest neighbors).
* **Options**:

  * ğŸ”¹ **Chroma** â†’ easy, LangChain-native, good for prototyping.
  * ğŸ”¹ **FAISS** (Meta) â†’ rock-solid, local, production-ready.
  * ğŸ”¹ **Weaviate**, **Qdrant**, **Milvus** â†’ server-grade, clustering, scaling.
  * ğŸ”¹ **ElasticSearch / OpenSearch** â†’ if you want hybrid sparse+dense in one place.
* **Best choice**:

  * âœ… **FAISS** if you want lightweight local + production stability.
  * âœ… **Chroma** if you want easy integration.
  * âœ… **Qdrant** if you want hybrid + REST API deployment.

---

## 5. **Chunker / Preprocessor**

* **What it does**: splits docs into manageable pieces (300â€“1000 tokens) so embeddings make sense.
* **Options**:

  * LangChain `RecursiveCharacterTextSplitter`
  * Custom (like your `cleaners.py`)
* **Best choice**:

  * âœ… Recursive chunking + overlap (\~200 tokens overlap).

---

## 6. **LLM (Generator)**

* **What it does**: consumes query + retrieved docs, generates answer.
* **Options**:

  * ğŸ”¹ **Ollama** (local hosting of LLaMA-3, Mistral, Gemma etc.).
  * ğŸ”¹ **OpenAI GPT-4o / 4-mini** â†’ strong, API-based.
  * ğŸ”¹ **LLaMA-3.2 (Meta)**, **Mistral 7B**, **Phi-3-mini** â†’ open, local, good.
* **Best choice**:

  * âœ… Ollama + **LLaMA-3.2-Instruct** (German + English).
  * âœ… For small RAM laptops â†’ `Phi-3-mini-4k-instruct`.

---

## 7. **Eval tools**

* **What it does**: checks if RAG answers are correct, faithful, relevant.
* **Options**:

  * ğŸ”¹ **RAGAS** (open-source, built for RAG evaluation).
  * ğŸ”¹ **TruLens** (trace-based).
  * ğŸ”¹ Human evals (gold-standard).
* **Best choice**:

  * âœ… RAGAS + a few golden test queries.

---

# ğŸ† Recommended Stack for You (offline, German, healthcare)

* **Embedder**: `BAAI/bge-m3` (multi-lingual, CPU/GPU friendly).
* **Retriever**: Hybrid (BM25 + dense) with `EnsembleRetriever`.
* **Reranker**: `BAAI/bge-reranker-v2-m3`.
* **Vector DB**: Chroma now â†’ FAISS or Qdrant later.
* **LLM**: Ollama with `LLaMA-3.2-3B-Instruct` or `Phi-3-mini`.
* **Eval**: RAGAS notebook in `/tests/`.

---

ğŸ“Œ In short:

* **Embed** â†’ semantic vectors.
* **Retriever** â†’ get candidates (BM25 + dense).
* **Reranker** â†’ refine candidates.
* **LLM** â†’ final answer.
* **Vector DB** â†’ the â€œmemoryâ€.

---


Hereâ€™s exactly what that RAGAS workflow does, step-by-step, and why each step exists.

---

# 1) Build an evaluation set from your FAQ

**File:** `tests/build_evalset_from_faq.py`
**Command:** `python -m tests.build_evalset_from_faq`

**What happens**

1. Load `data/faq_de.yaml`.
2. For each FAQ item, take:

   * `question` â†’ becomes the **eval question**.
   * `answer` â†’ becomes the **ground truth** (wrapped as a one-item list, as RAGAS expects `ground_truths: List[str]`).
3. Shuffle and keep N items (default 20) so the run is quick.
4. Write out **one JSON line per sample** to `tests/eval_set.jsonl` with:

   ```json
   {"question": "...", "ground_truths": ["..."]}
   ```

**Why**

* You need a *reference* answer to judge model outputs against. Your FAQ gives you high-precision ground truth.

---

# 2) Collect model answers and the contexts used

**File:** `tests/collect_answers.py`
**Command:** `python -m tests.collect_answers`

**What happens**

1. Load the compiled **LangGraph**: `graph = build_graph()`.
2. For each row in `tests/eval_set.jsonl`:

   * Create `RAGState(question=q)` and call `graph.invoke(state)`.
   * This triggers your full pipeline: guardrails â†’ `HybridRetriever` (BM25 + dense + rerank) â†’ LLM generation with your prompt.
   * Capture:

     * `answer` â†’ the modelâ€™s generated text.
     * `docs` (top-k retrieved chunks) â†’ extract their `text` as **contexts** (RAGAS expects `List[str]`).
3. Write **one JSON line per sample** to `tests/eval_set_with_preds.jsonl` with:

   ```json
   {
     "question": "...",
     "ground_truths": ["..."],
     "answer": "...",
     "contexts": ["chunk1 text...", "chunk2 text...", ...]
   }
   ```

**Why**

* RAGAS metrics evaluate two things:

  1. how well the **answer** matches the **ground truth** and is supported by **contexts**, and
  2. how relevant/complete the **contexts** are for the **question**.
* Therefore you must provide **both** the generated answer and the **exact contexts** you fed the LLM.

---

# 3) Score with RAGAS

**File:** `tests/ragas_eval.py`
**Command:** `python -m tests.ragas_eval`

**What happens**

1. Load `tests/eval_set_with_preds.jsonl` into a Hugging Face `Dataset`.
2. Instantiate **offline judges**:

   * `judge_llm`: a local LLM via `ChatOllama(model="llama3.2:3b", temperature=0.0)`.
     *Used by RAGAS to judge semantic qualities (e.g., â€œdoes this answer address the question?â€).*
   * `embeddings`: `BAAI/bge-m3` via `HuggingFaceEmbeddings` (normalize on).
     *Used by RAGAS to compute semantic similarity between answer/ground truth/context.*
3. Run `ragas.evaluate(...)` with chosen metrics:

   * **faithfulness**: is the answer supported by the provided **contexts** (grounding)?
   * **answer_relevancy**: does the answer address the **question**?
   * **context_precision**: how relevant are the **returned contexts** to the **question**? (penalizes extra/noisy passages)
   * **context_recall**: did your retrieval include the **necessary** context to answer the question? (penalizes missing key passages)
4. Print the **mean** of each metric and save per-sample details to `tests/ragas_scores.csv`.

**Why**

* Faithfulness tells you if the generator uses the retrieved text (low = hallucinating or off-context).
* Context precision/recall tells you if the **retriever** is over- or under-fetching relevant chunks.
* Answer relevancy checks if the output actually answers the question.

---

# 4) Interpreting scores (quick heuristics)

* **Faithfulness** is your â€œno hallucinationâ€ signal. If low, tighten prompts (stricter grounding), reduce top-k, or fix retrieval noise/reranker.
* **Context precision** low â†’ your top-k includes irrelevant chunks (noise). Consider stronger reranking, lower `top_k`, better cleaners/chunking.
* **Context recall** low â†’ retriever missed key chunks. Consider higher `top_k`, adjust BM25/dense weights, improve embeddings or tokenization.
* **Answer relevancy** low â†’ LLM didnâ€™t address the question; improve prompt, reduce distractions (fewer/noisier contexts), lower temperature, or pick a model with better instruction following.

---

# 5) Why itâ€™s offline-friendly

* The **judge LLM** is your **local Ollama model** (no external API).
* The **embeddings** are computed locally via `HuggingFaceEmbeddings`.
* Everything runs inside your `env_aok_chatbot` environment.

---

# 6) Typical ablation workflow (how to use RAGAS to improve)

1. Baseline run:

   * `build_evalset_from_faq` â†’ `collect_answers` â†’ `ragas_eval`
2. Change one thing:

   * e.g., `top_k` from 4 â†’ 6, or disable `bge-reranker`, or switch BM25 weight.
3. Re-run `collect_answers` (to regenerate answers/contexts) â†’ `ragas_eval`.
4. Compare `ragas_scores.csv` means across runs. Keep a small table of:

   * config (top-k, reranker on/off, prompt variant)
   * faithfulness, context_precision, context_recall, answer_relevancy.

---

# 7) Common pitfalls (and fixes)

* **Answers include a â€œQuellen:â€ block generated by the model** â†’ your `graph.py` already strips model-specified sources; good. Keep it to avoid confusing RAGAS.
* **Contexts are empty for some questions** â†’ retrieval failed; your `retrieve` node returns a fallback answer. Those items will hurt recall/faithfulness; investigate source coverage or chunking.
* **Inconsistent results run-to-run** â†’ set temperature to **0.0** for both the judge LLM and your generation LLM during eval.
* **Slow judging** â†’ evaluate on **10â€“20** items first; expand later. Use smaller judge model if needed.

---

## TL;DR

* **Step 1** turns your FAQ into an **eval set** (questions + ground truths).
* **Step 2** runs your **actual pipeline** to produce answers **and the exact contexts** used.
* **Step 3** uses RAGAS (with local judges) to score faithfulness, context precision/recall, and answer relevancy.
* Use the scores to tune retrieval (`top_k`, reranker), prompts, and chunkingâ€”repeat until the metrics look strong.

RAG Chatbot Pipeline:
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Data Sources       â”‚
                â”‚  (AOK site, PDFs, FAQâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Ingestion        â”‚
                â”‚ ingest/ingest.py     â”‚
                â”‚ + cleaners.py        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Chunker       â”‚
                  â”‚ (split text)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Embedder       â”‚
                  â”‚ BAAI/bge-m3    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Vector DB (Dense Index)       â”‚
          â”‚   Chroma (data/processed/chroma)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Hybrid Retriever (app/rag/retriever.py)         â”‚
 â”‚  â€¢ BM25Retriever (sparse, chunks.jsonl)         â”‚
 â”‚  â€¢ DenseRetriever (Chroma vectors)              â”‚
 â”‚  â†’ EnsembleRetriever (fusion, weights)          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Reranker     â”‚
                  â”‚ bge-reranker   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    LLM (Ollama)â”‚
                  â”‚ LLaMA-3.2 / Phiâ”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ LangGraph      â”‚
                  â”‚ graph.py       â”‚
                  â”‚ (retrievalâ†’gen)â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  Answer + Citations     â”‚
             â”‚  Streamlit / FastAPI    â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



ğŸ§ Your retriever returns high-similarity junkâ€”how do you improve contextual precision? What do you do?

In Retrieval-Augmented Generation (RAG), one of the most common failure modes is this:
The retriever confidently returns passages that look similar to the query but are contextually irrelevant.

Example:
ğŸ”¹ Query â†’ â€œHow do I enable two-factor authentication in Gmail?â€
ğŸ”¹ Retriever â†’ returns docs about â€œmulti-factor auth in Microsoft Teams.â€
High overlap, wrong context.

This is a precision problem: lots of near-matches, few truly useful hits.

Hereâ€™s how strong teams improve contextual precision:
1ï¸) Better chunking strategies â†’ Ensure documents are split semantically, not arbitrarily.
2ï¸) Query rewriting / expansion (HyDE, query routing) â†’ Make queries more precise before retrieval.
3ï¸) Use rerankers â†’ Add a lightweight ranking step that scores passages by contextual relevance, not just embedding similarity.
4ï¸) Task-specific embeddings â†’ Fine-tune embeddings on your domain instead of relying on generic off-the-shelf vectors.

ğŸ’¡ RAG isnâ€™t about fetching â€œsimilarâ€ text. Itâ€™s about fetching the right text. Without contextual precision, your generator is building on junk.

Smart Chunking: Adaptive + Semantic; Advanced Reranking; Incremental Re-Embedding (FAISS, HNSW); Hybrid Retrieval (KWs + Vectors); Guardrails & Fact-Checking

Why metrics are failing?
Hereâ€™s the crisp, interview-ready answer:

**Why it failed:**
â€œThe failures werenâ€™t in my RAG pipelineâ€”they were in the *evaluation step*. RAGAS uses an LLM-as-judge that must return strict JSON. I was judging with a small 3B base model which occasionally echoed the few-shot examples (the â€˜Einsteinâ€™ text) instead of emitting valid JSON, and concurrent calls on my laptop caused timeouts. Early on I also hit a RAGAS/LangChain version-schema mismatch, which I fixed.â€

**What I changed to stabilize it:**

* Pinned/aligned deps and switched to the RAGAS â‰¥0.2 schema (`user_input`, `reference`, `response`, `retrieved_contexts`).
* Forced JSON output (`format="json"`) + strict decoding (temp=0, top_k=1, top_p=0.1, short outputs).
* Used the **instruct** variant of the same 3B model and serialized judge calls (`OLLAMA_NUM_PARALLEL=1`) to avoid timeouts.
* Validated/capped contexts (list[str], dedup, length limit) and ensured non-empty `reference`.
* Made evaluation tolerant to single-row judge failures (`raise_exceptions=False`) and reported aggregates.

**How Iâ€™d prevent it in production:**
â€œKeep the RAG model, but use a more deterministic judge (instruct model/API), add a JSON schema validator with auto-retry, and run preflight checks on data shapes. For retrieval quality, complement LLM-judged metrics with embedding-based ones to reduce dependence on a chat modelâ€™s formatting.â€

â€œThe NaNs arenâ€™t from my RAG outputs; theyâ€™re from the LLM-as-judge sometimes failing to emit strict JSON. I fixed it by aligning RAGAS v0.2 schema, using the instruct variant of the 3B judge with format="json" and deterministic decoding, serializing judge calls, and trimming contexts. After that, all metrics compute reliably; remaining NaNs (if any) are isolated row-level judge timeouts and donâ€™t affect the aggregate.â€

4) Interpreting scores (quick heuristics)

Faithfulness is your â€œno hallucinationâ€ signal. If low, tighten prompts (stricter grounding), reduce top-k, or fix retrieval noise/reranker.

Context precision low â†’ your top-k includes irrelevant chunks (noise). Consider stronger reranking, lower top_k, better cleaners/chunking.

Context recall low â†’ retriever missed key chunks. Consider higher top_k, adjust BM25/dense weights, improve embeddings or tokenization.

Answer relevancy low â†’ LLM didnâ€™t address the question; improve prompt, reduce distractions (fewer/noisier contexts), lower temperature, or pick a model with better instruction following.